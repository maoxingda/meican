## Tutorial Taskflow API

That's it, we are done! We have invoked the Extract task, obtained the order data from there and sent it over to the Transform task for summarization, and then invoked the Load task with the summarized data. <font color=red>The dependencies between the tasks and the passing of data between these tasks which could be running on different workers on different nodes on the network is all handled by Airflow.</font>

Airflow sends out Tasks to run on Workers as space becomes available, so there's no guarantee all the tasks in your DAG will run on the same worker or the same machine.

All of the Xcom usage for data passing between these tasks is abstracted away from the DAG author in Airflow 2.0. However, Xcom variables are used behind the scenes and can be viewed using the Airflow UI as necessary for debugging or DAG monitoring.

Similarly, task dependencies are automatically generated within TaskFlows based on the functional invocation of tasks. In Airflow 1.x, tasks had to be explicitly created and dependencies specified as shown below.

In contrast, with the Taskflow API in Airflow 2.0, the invocation itself automatically generates the dependencies.

## Adding dependencies to decorated tasks from regular tasks

The above tutorial shows how to create dependencies between python-based tasks. However, it is quite possible while writing a DAG to have some pre-existing tasks such as [`BashOperator`](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html#airflow.operators.bash.BashOperator) or [`FileSensor`](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/filesystem/index.html#airflow.sensors.filesystem.FileSensor) based tasks which need to be run first before a python-based task is run.

Building this dependency is shown in the code below:

```python
@task()
def extract_from_file():
    """
    #### Extract from file task
    A simple Extract task to get data ready for the rest of the data
    pipeline, by reading the data from a file into a pandas dataframe
    """
    order_data_file = '/tmp/order_data.csv'
    order_data_df = pd.read_csv(order_data_file)


file_task = FileSensor(task_id='check_file', filepath='/tmp/order_data.csv')
order_data = extract_from_file()

file_task >> order_data
```

In the above code block, a new python-based task is defined as `extract_from_file` which reads the data from a known file location. In the main DAG, a new `FileSensor` task is defined to check for this file. Please note that this is a Sensor task which waits for the file. Finally, a dependency between this Sensor task and the python-based task is specified.

## TaskFlow

If you write most of your DAGs using plain Python code rather than Operators, then the TaskFlow API will make it much easier to author clean DAGs without extra boilerplate, all using the `@task` decorator.

TaskFlow takes care of moving inputs and outputs between your Tasks using XComs for you, as well as automatically calculating dependencies - when you call a TaskFlow function in your DAG file, rather than executing it, you will get an object representing the XCom for the result (an `XComArg`), that you can then use as inputs to downstream tasks or operators. 

## [Python function decorator](https://github.com/casassg/corrent/blob/master/corrent/decorators.py)

- Should also allow to pass <font color=red>args/kwargs</font> to the PythonOperator from the decorator <font color=red>args/kwargs</font>. Ex: Set *task_id* for the operator *<font color=red>task(task_id='random')</font>*.
- Add <font color=red>*multiple_outputs*</font> flag that unrolls a dictionary into several keyed XCom values. 
- 2 ways to use it:
  - *<font color=blue>@dag.task:</font>* Uses dag object, does not need the DAG context, task automatically assigned to DAG.
  - *<font color=blue>@airflow.decorators.task:</font>* Makes function an operator, but does not automatically assign it to a DAG (<font color=red>unless declared inside a DAG context</font>)
- Make it easier to set *<font color=red>op_arg</font>* and *<font color=red>op_kwargs</font>* from <font color=red>`__call__`</font>, effectively enabling function like operations based on XCom values. 
- Should allow multiple calls in the same DAG or a different DAG, generating new task ids programmatically if needed.
- Add *<font color=red>get_current_context</font>* to get current Airflow context in a decorated function.

## Are there any downsides to this change?

- Potentially, this could lead to more users/customers using XCom to pass around large pieces of data.
- XCom uses pickling for serialization/deserialization, which may be unstable for certain data passed around. 

